本项目是基于 3D Gaussian Splatting 的一个开源实现，旨在为一个已经重建好的3D高斯场景高效地注入一个开放词汇的语义场。作为语义3dgs的基础框架，功能上以及效果上相对较差，可作为一个简单的框架使用。
该方法的核心思想是，利用强大的预训练视觉-语言模型(CLIP)作为语义知识的来源，通过主成分分析(PCA)构建一个稳定且紧凑的语义监督空间，最终以微调的方式为场景中的每个三维高斯基元赋予可学习的语义特征。

核心特性
开放词汇查询: 能够响应任意文本描述，而不仅限于预设的类别。

后期处理: 无需重新训练3DGS的几何和颜色，直接在已有的模型上进行语义微调。

低额外开销: 仅为每个点增加一个16维的浮点特征，对模型体积影响较小。

方法简洁: 整个流程不涉及复杂的网络结构或损失函数，易于理解和实现。


管线分为两个主要阶段：

监督信号生成: 我们首先为训练视图生成像素级的真值(Ground Truth)语义图。

使用**Segment Anything Model (SAM)**对每张训练图像进行过分割，得到高质量的物体掩码。

对每个掩码，使用CLIP图像编码器提取其对应的512维高维特征向量。

收集整个数据集中所有掩码的CLIP特征，使用**主成分分析(PCA)**学习一个从512维到16维的投影。

将所有掩码的特征投影到这个16维空间，最终为每张训练图生成一张16通道的真值语义图。

语义微调:

为原始3D高斯模型中的每个点，增加一个随机初始化的16维可学习语义特征f。

冻结所有与几何、颜色相关的原始参数（位置、旋转、不透明度、球谐系数）。

使用一个与颜色渲染并行的可微分语义渲染器，将3D点的语义特征f渲染到2D图像上。

使用简单的L1损失函数，度量渲染语义图与真值语义图之间的差异，并仅更新语义特征
